{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>ERM with DNN under penalty of Equalized Odds</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement here a regular Empirical Risk Minimization (ERM) of a Deep Neural Network (DNN) penalized to enforce an Equalized Odds constraint. More formally, given a dataset of size $n$ consisting of context features $x$, target $y$ and a sensitive information $z$ to protect, we want to solve\n",
    "$$\n",
    "\\text{argmin}_{h\\in\\mathcal{H}}\\frac{1}{n}\\sum_{i=1}^n \\ell(y_i, h(x_i)) + \\lambda \\chi^2\n",
    "$$\n",
    "where $\\ell$ is for instance the MSE and the penalty is\n",
    "$$\n",
    "\\chi^2 = \\chi^2\\left(\\hat{\\pi}(h(x), z), \\hat{\\pi}(h(x))\\otimes\\hat{\\pi}(z)\\right)\n",
    "$$\n",
    "where $\\hat{\\pi}$ denotes the empirical density estimated through a Gaussian KDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "\n",
    "We use here the _communities and crimes_ dataset that can be found on the UCI Machine Learning Repository (http://archive.ics.uci.edu/ml/datasets/communities+and+crime). Non-predictive information, such as city name, state... have been removed and the file is at the arff format for ease of loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('../..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.data_loading import read_dataset\n",
    "x_train, y_train, z_train, x_test, y_test, z_test = read_dataset(name='crimes', fold=1)\n",
    "n, d = x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Deep Neural Network\n",
    "\n",
    "We define a very simple DNN for regression here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NetRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NetRegression, self).__init__()\n",
    "        size = 50\n",
    "        self.first = nn.Linear(input_size, size)\n",
    "        self.fc = nn.Linear(size, size)\n",
    "        self.last = nn.Linear(size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.selu(self.first(x))\n",
    "        out = F.selu(self.fc(out))\n",
    "        out = self.last(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fairness-inducing regularizer\n",
    "We implement now the regularizer. The empirical densities $\\hat{\\pi}$ are estimated using a Gaussian KDE.\n",
    "$$\n",
    "\\chi^2 = \\chi^2\\left(\\hat{\\pi}(h(x), z), \\hat{\\pi}(h(x))\\otimes\\hat{\\pi}(z)\\right)\n",
    "$$\n",
    "This used to enforce the independence $X \\perp Y$.\n",
    "Practically, we will want to enforce $\\text{prediction} \\perp \\text{sensitive}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facl.independence.density_estimation.pytorch_kde import kde\n",
    "from facl.independence.hgr import chi_2\n",
    "\n",
    "def chi_squared_l1_kde(X, Y):\n",
    "    return chi_2(X, Y, kde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fairness-penalized ERM\n",
    "\n",
    "We now implement the full learning loop. The regression loss used is the quadratic loss with a L2 regularization and the fairness-inducing penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "def regularized_learning(x_train, y_train, z_train, model, fairness_penalty, lr=1e-5, num_epochs=10):\n",
    "    # wrap dataset in torch tensors\n",
    "    Y = torch.tensor(y_train.astype(np.float32))\n",
    "    X = torch.tensor(x_train.astype(np.float32))\n",
    "    Z = torch.tensor(z_train.astype(np.float32))\n",
    "    dataset = data_utils.TensorDataset(X, Y, Z)\n",
    "    dataset_loader = data_utils.DataLoader(dataset=dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "    # mse regression objective\n",
    "    data_fitting_loss = nn.MSELoss()\n",
    "\n",
    "    # stochastic optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "\n",
    "    for j in range(num_epochs):\n",
    "        for i, (x, y, z) in enumerate(dataset_loader):\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x).flatten()\n",
    "                loss = data_fitting_loss(outputs, y)\n",
    "                loss += fairness_penalty(outputs, z)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            optimizer.step(closure)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "For the evaluation on the test set, we compute two metrics: the MSE (accuracy) and HGR$|_\\infty$ (fairness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facl.independence.hgr import hgr\n",
    "\n",
    "def evaluate(model, x, y, z):\n",
    "    Y = torch.tensor(y.astype(np.float32))\n",
    "    Z = torch.Tensor(z.astype(np.float32))\n",
    "    X = torch.tensor(x.astype(np.float32))\n",
    "\n",
    "    prediction = model(X).detach().flatten()\n",
    "    loss = nn.MSELoss()(prediction, Y)\n",
    "    hgr_val = hgr(prediction, Z, kde)\n",
    "    return loss.item(), hgr_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running everything together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetRegression(d, 1)\n",
    "\n",
    "num_epochs = 20\n",
    "lr = 1e-5\n",
    "\n",
    "# $\\chi^2|_1$\n",
    "penalty_coefficient = 1.0\n",
    "penalty = chi_squared_l1_kde\n",
    "\n",
    "model = regularized_learning(x_train, y_train, z_train, model=model, fairness_penalty=penalty, lr=lr, \\\n",
    "                             num_epochs=num_epochs)\n",
    "\n",
    "mse, hgr_infty = evaluate(model, x_test, y_test, z_test)\n",
    "print(\"MSE:{} HGR_infty:{}\".format(mse, hgr_infty))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
